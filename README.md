# vision-language

1. Efficient Estimation of Word Representations in Vector Space (2013)

2. Attention Is All You Need (2017)

3. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (2018)

4. Language Models are Unsupervised Multitask Learners (2018)

5. T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (2019)

6. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension (2019)

7. Language Models are Few-Shot Learners (2020)

8. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (2020)

9. CLIP: Connecting Text and Images (2021)

10. ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision (2021)

11. ALIGN: Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision (2021)

12. BEiT: BERT Pre-Training of Image Transformers (2021)

13. LoRA: Low-Rank Adaptation of Large Language Models (2021)

14. Flamingo: a Visual Language Model for Few-Shot Learning (2022)

15. BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation (2022)

16. LLaMA: Open and Efficient Foundation Language Models (2023)
